{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ppDQjTFtlHU",
        "outputId": "020bef60-6fd0-42f2-bb5d-aed1c9e4cb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n",
            "  Downloading tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (2.2.0)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.19.6)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.29.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (15.0.6.1)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (21.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.51.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (4.4.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow_text) (1.14.1)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.16.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.25.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (5.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (6.0.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow_text) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, keras, tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-23.1.21 keras-2.11.0 tensorboard-2.11.2 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow_text-2.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_text\n",
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import string\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from itertools import groupby\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"category_df.csv\")"
      ],
      "metadata": {
        "id": "jRQVvXPFtm4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Rs51SStm1O",
        "outputId": "fdbfb104-7cea-4147-978d-1e363eb19d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45904, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVWoBUavtmsx",
        "outputId": "a83f6e85-9a38-470a-f381-f75aa9c431f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "reviewId                    0\n",
              "content                     0\n",
              "score                       0\n",
              "thumbsUpCount               0\n",
              "reviewCreatedVersion     5093\n",
              "at                          0\n",
              "replyContent            43427\n",
              "repliedAt               43427\n",
              "predicted_category          0\n",
              "sentiment                   0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Sqfb-3pEvK0n",
        "outputId": "17d09f45-4cce-42e2-e963-c89b27bd7d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content         category\n",
              "0                                               woww  USER_EXPERIENCE\n",
              "1                let me know more details about this          CONTENT\n",
              "2  i've been using this for a while and there's a...        INTERFACE\n",
              "3                                               good  USER_EXPERIENCE\n",
              "4                                             mjkobe  USER_EXPERIENCE"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c295261-a8ae-46fd-b185-aa5f6c3fbedb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>woww</td>\n",
              "      <td>USER_EXPERIENCE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>let me know more details about this</td>\n",
              "      <td>CONTENT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i've been using this for a while and there's a...</td>\n",
              "      <td>INTERFACE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>good</td>\n",
              "      <td>USER_EXPERIENCE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mjkobe</td>\n",
              "      <td>USER_EXPERIENCE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c295261-a8ae-46fd-b185-aa5f6c3fbedb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c295261-a8ae-46fd-b185-aa5f6c3fbedb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c295261-a8ae-46fd-b185-aa5f6c3fbedb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns = ['category'], drop_first=False)"
      ],
      "metadata": {
        "id": "1oV5yXeSvani"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Výběr 30 % dat pro zkrácení doby trénování modelu.  \n",
        "Dříve bylo provedeno mnoho pokusů o trénování modelu na více datech, ale  bezplatná verze GoogleColab není určena pro dlouhé vykonávání kódu, tyto pokusy byly neúspěšné. "
      ],
      "metadata": {
        "id": "OQH7bsh-zMnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0.3\n",
        "short_dataset = df.sample(frac = p, random_state = 42)\n",
        "\n",
        "# Spliting\n",
        "p = 0.8\n",
        "train = short_dataset.sample(frac = p, random_state = 42)\n",
        "test = short_dataset.drop(train.index)"
      ],
      "metadata": {
        "id": "2g0Kb2WNve-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgnpgmzyvlDK",
        "outputId": "b4902a4d-67e6-48e7-baf0-d80675296bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11017, 5)\n",
            "(2754, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Removing duplicates"
      ],
      "metadata": {
        "id": "hMS0muq_vowL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Odstranění duplicit, pokud se slova v komentáři vyskytují více než 2krát.\n",
        "def remove_duplicates(text_before):\n",
        "    my_dict = dict()\n",
        "    text_after = list()\n",
        "    for word in text_before.split():\n",
        "        if word not in my_dict.keys():\n",
        "            my_dict[word] = 1\n",
        "        else:\n",
        "            my_dict[word] = my_dict[word] + 1\n",
        "    \n",
        "    for key,value in my_dict.items():\n",
        "        if value>=2:\n",
        "            text_after.append(key)\n",
        "        else:\n",
        "            text_after.append(key)\n",
        "    return \" \".join(text_after)"
      ],
      "metadata": {
        "id": "KS39E8NOvpU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Délka komentářů"
      ],
      "metadata": {
        "id": "m39LlrkxvuOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train['comment_text_clean'] = train['content'].apply(lambda text : remove_duplicates(text))\n",
        "test['comment_text_clean'] = test['content'].apply(lambda text : remove_duplicates(text))\n",
        "# délka komentářů\n",
        "train['comment_text_len'] = train['comment_text_clean'].apply(lambda x : len(x.split())) "
      ],
      "metadata": {
        "id": "JuOknZ0evujV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['comment_len'] = train['comment_text_clean'].str.len()\n",
        "fig = px.histogram(train, x = 'comment_len',\n",
        "                   title='Histogram of len of comments',\n",
        "                   labels={'Number of samples, len of comments'},\n",
        "                   opacity = 0.8,\n",
        "                   log_y=True,\n",
        "                   color_discrete_sequence = ['blue'])\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "EWz3SuPVvv_9",
        "outputId": "2acecb17-7f18-4de3-979d-8dc4c9504737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"59b45b98-4530-478a-a72b-4acd25f5b33d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"59b45b98-4530-478a-a72b-4acd25f5b33d\")) {                    Plotly.newPlot(                        \"59b45b98-4530-478a-a72b-4acd25f5b33d\",                        [{\"alignmentgroup\":\"True\",\"bingroup\":\"x\",\"hovertemplate\":\"comment_len=%{x}<br>count=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"blue\",\"opacity\":0.8,\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[11,77,30,70,65,4,18,7,4,70,69,82,38,11,237,37,8,2,5,7,77,39,350,28,125,89,9,4,12,25,117,44,4,95,42,233,4,146,124,4,130,10,11,183,20,7,37,114,7,36,9,17,76,152,114,37,88,68,91,18,36,26,28,10,56,38,7,42,4,9,34,4,42,14,142,25,64,125,33,76,14,25,88,83,23,20,27,34,213,4,41,11,99,18,13,71,34,32,57,58,10,391,269,9,127,13,12,52,31,10,36,160,9,2,44,71,55,18,101,13,4,36,44,7,12,35,24,30,63,57,74,28,70,112,570,12,10,28,125,282,9,43,269,48,36,11,51,141,21,20,23,35,27,6,35,288,40,204,49,39,14,6,48,81,13,4,30,142,13,42,15,23,32,238,42,199,4,27,18,47,14,50,9,55,151,24,48,7,43,13,29,14,53,19,60,46,100,72,98,8,80,204,8,25,38,225,20,21,112,24,159,39,27,19,14,10,52,59,4,84,4,12,94,358,81,45,90,7,51,4,34,66,37,61,4,7,190,5,6,51,195,27,162,35,38,173,25,82,51,18,46,51,9,53,12,64,49,43,33,56,15,73,5,5,25,92,92,5,4,94,144,32,98,33,45,187,14,47,36,317,12,44,4,67,6,14,64,40,123,29,29,65,59,46,87,18,80,9,249,81,9,59,112,125,13,170,94,49,4,13,9,15,2,65,61,10,18,41,129,92,46,27,21,10,91,48,9,136,23,85,121,50,15,20,8,11,8,92,442,8,29,4,8,8,227,4,119,91,97,5,10,7,12,144,18,29,13,94,60,45,40,37,35,38,48,24,76,35,202,41,176,242,56,6,21,22,4,4,27,4,60,8,438,8,91,42,98,40,70,33,17,30,122,62,42,8,115,39,88,79,77,4,48,5,4,284,118,169,63,3,15,55,33,16,35,7,189,74,26,4,123,85,6,27,34,14,18,4,82,77,5,309,15,11,9,14,4,68,10,11,102,38,19,65,16,119,4,17,8,83,18,3,24,32,47,35,27,55,65,53,197,104,231,113,39,121,64,14,170,73,6,17,51,47,141,84,123,31,240,4,54,96,73,59,41,48,47,373,21,7,17,10,58,109,73,4,8,226,11,179,69,70,194,73,168,3,11,413,109,9,213,10,124,20,234,77,27,21,66,16,14,109,19,20,54,59,41,5,25,24,9,35,9,111,283,35,16,119,38,43,9,118,99,15,4,2,9,11,4,193,4,67,121,171,29,14,10,11,43,92,41,42,6,36,11,21,75,5,44,24,40,8,78,25,326,9,243,19,43,169,54,68,52,4,4,13,34,91,8,12,97,55,76,208,36,13,22,179,92,26,126,6,7,4,82,10,17,7,160,29,61,8,6,105,432,9,4,7,4,24,37,4,22,101,178,101,8,67,8,66,202,9,30,28,4,308,42,22,4,9,123,62,42,36,121,83,105,226,47,65,60,425,140,94,42,45,3,9,46,94,49,4,13,255,13,171,14,79,9,17,13,12,18,43,108,12,8,13,70,25,173,7,9,26,147,15,4,318,26,10,49,26,388,14,17,8,121,64,27,15,304,30,199,19,63,62,101,15,28,4,58,5,43,152,12,190,97,32,123,17,128,107,36,16,230,28,70,95,83,57,8,32,420,32,54,4,21,55,32,4,7,186,5,22,8,27,56,167,66,135,358,8,2,15,37,15,48,56,204,13,35,13,6,15,93,100,82,8,2,74,37,161,14,40,62,8,144,7,34,16,34,115,22,67,121,89,4,34,20,4,129,180,106,7,10,144,49,60,18,153,38,189,107,49,43,98,51,55,240,14,91,121,82,13,82,39,228,15,403,7,9,4,14,75,16,291,17,172,63,272,62,21,70,220,41,12,16,83,9,13,15,5,9,89,27,5,72,12,6,37,64,38,68,9,190,101,4,10,16,69,46,19,21,20,39,8,18,76,79,22,16,24,81,39,15,8,48,7,25,17,59,16,58,15,9,26,109,47,8,30,122,185,245,17,8,17,9,81,9,7,9,23,51,36,14,97,10,27,17,31,6,144,25,4,13,149,146,4,7,50,359,144,4,14,8,28,9,40,9,24,123,23,107,35,16,7,5,19,11,34,29,60,4,9,5,143,122,36,16,18,109,151,80,33,63,9,21,263,5,195,47,86,5,9,45,58,89,14,185,10,69,3,10,48,18,24,9,14,15,75,19,5,9,89,19,27,54,87,40,76,34,49,141,11,49,19,10,4,18,12,70,240,30,257,15,219,44,40,8,34,4,65,20,191,46,36,13,4,16,47,19,130,39,390,15,15,44,19,7,4,10,256,7,72,44,61,17,54,5,4,15,8,177,98,14,8,4,11,111,13,130,202,62,57,4,31,117,64,9,17,148,17,8,64,82,30,37,98,4,4,7,48,170,16,58,124,85,21,76,53,43,147,17,50,93,87,437,156,7,35,112,79,31,15,119,12,18,15,25,13,13,35,193,24,160,10,436,18,34,59,74,81,49,2,8,2,171,4,39,59,57,17,167,57,32,48,4,169,5,47,138,28,7,121,91,45,295,453,15,62,48,105,8,38,23,48,81,98,167,12,174,121,148,88,243,13,9,4,67,9,9,161,17,62,101,91,126,27,35,51,9,5,29,14,4,47,81,32,15,55,51,4,231,69,71,9,226,4,18,14,5,22,9,97,11,41,11,7,19,27,71,94,7,102,92,55,9,200,31,17,43,64,54,62,130,28,77,93,246,9,14,33,68,17,47,157,71,142,11,11,13,8,67,9,4,42,13,8,35,11,75,81,7,19,15,4,43,36,187,128,200,10,80,14,7,13,55,34,51,15,77,54,84,63,2,7,181,15,149,81,16,73,84,17,61,4,65,70,11,18,61,9,65,14,43,8,33,153,54,15,115,56,5,100,9,13,7,254,14,7,2,110,12,13,117,26,9,13,128,37,68,9,5,4,27,10,20,111,99,50,35,9,9,27,7,121,4,74,35,228,50,114,63,15,4,59,43,64,30,9,90,50,75,79,180,51,16,123,15,178,12,8,253,4,51,157,27,8,35,59,15,89,60,2,47,14,14,31,12,4,31,11,73,35,442,13,41,54,87,157,29,3,14,56,72,11,9,24,57,26,15,341,15,14,30,18,37,125,26,9,46,9,46,377,48,66,20,21,43,56,67,169,30,38,15,40,92,24,283,66,44,65,8,59,10,456,10,104,56,232,20,22,96,115,14,28,107,46,30,93,37,107,55,189,16,83,26,10,5,90,88,67,25,62,15,88,45,102,38,58,5,155,7,52,259,41,32,80,154,10,27,42,8,9,14,133,130,172,9,33,9,35,3,28,58,116,4,42,9,4,92,44,18,63,16,91,47,21,20,4,32,20,335,13,83,4,196,18,81,68,47,123,119,257,4,4,25,196,15,67,54,21,9,54,84,32,52,22,23,43,10,77,202,32,111,108,60,4,63,45,27,208,43,42,22,25,7,22,116,88,44,149,8,4,43,58,4,33,187,16,8,57,223,11,299,59,50,19,40,117,37,101,8,47,46,82,10,8,10,4,4,23,13,13,11,78,106,10,78,53,48,31,56,43,24,8,20,4,230,13,38,8,5,20,63,8,28,67,108,143,13,21,19,24,9,115,26,16,33,8,8,19,55,27,16,28,14,32,64,8,118,191,15,7,164,105,36,106,98,58,152,34,4,101,74,7,33,14,70,23,112,56,26,4,50,63,31,35,25,9,70,135,10,89,49,299,33,18,137,75,51,65,49,9,167,7,368,8,19,18,75,19,8,22,80,5,71,35,87,15,53,53,23,35,4,21,61,51,4,30,89,69,56,43,83,26,389,31,4,9,45,15,81,15,34,194,20,66,46,8,8,244,31,168,39,17,17,157,28,13,14,9,38,61,195,55,121,11,269,6,164,48,177,172,5,4,10,9,11,43,17,98,25,31,36,81,13,59,160,4,3,30,25,35,26,14,49,22,17,71,22,41,16,41,11,49,13,76,147,37,7,94,12,25,8,25,10,40,12,95,46,11,9,6,42,25,11,157,84,32,128,9,10,4,8,124,13,42,16,10,33,36,61,262,45,20,88,27,220,4,64,107,29,31,23,156,16,26,104,36,30,56,2,9,9,20,118,61,27,7,20,4,4,16,252,31,39,12,106,39,268,73,88,8,28,8,87,140,82,16,338,43,17,16,119,16,39,134,61,4,51,7,127,35,47,15,33,95,80,27,99,49,11,19,4,94,133,12,3,138,95,18,31,76,81,8,36,49,43,81,4,10,214,6,30,66,31,277,12,113,2,124,32,4,321,8,16,4,11,16,89,17,38,256,74,6,8,4,90,8,23,9,382,15,76,16,16,4,264,109,40,91,4,16,82,44,147,7,31,91,60,102,52,9,22,227,75,4,10,38,62,63,8,178,86,93,71,59,197,41,4,14,7,160,15,4,12,29,9,5,5,16,49,19,11,68,91,4,7,5,5,14,11,45,160,12,225,83,15,20,82,34,9,19,8,113,36,9,9,91,5,39,100,159,77,50,33,4,24,53,4,4,8,23,23,11,64,228,17,42,161,46,87,101,89,23,4,19,4,86,44,73,10,47,80,397,21,4,150,186,8,130,150,34,10,59,4,28,73,10,67,14,82,19,56,22,136,202,46,13,9,71,4,23,53,7,12,60,73,52,4,35,63,51,51,141,30,194,30,62,158,115,20,52,17,11,3,58,16,51,8,29,65,9,8,91,15,9,24,61,7,8,4,72,71,72,14,9,147,146,104,36,31,4,16,9,106,9,45,27,246,118,43,30,10,3,32,194,47,69,417,37,58,39,33,16,70,62,9,26,15,20,54,51,19,89,29,53,98,119,129,103,12,43,36,10,65,82,124,155,180,74,42,88,21,45,79,3,74,4,38,29,8,302,25,41,42,134,103,20,9,41,37,83,134,4,9,44,5,37,14,22,4,8,35,13,22,5,46,87,11,9,20,129,111,16,247,138,43,26,10,287,49,36,220,26,37,35,76,31,163,28,62,84,4,51,48,8,92,31,30,29,33,13,16,169,55,11,5,45,44,8,49,162,50,12,39,4,4,8,7,68,8,138,11,132,95,64,9,16,76,184,27,8,30,13,50,4,22,99,40,36,29,231,16,94,131,81,9,10,34,8,5,26,15,13,38,12,9,176,332,265,35,43,320,25,4,4,66,152,85,62,140,9,80,35,102,13,30,24,4,49,19,6,11,21,24,89,94,8,9,182,14,30,75,49,222,65,9,293,46,6,46,13,7,79,81,195,3,21,144,57,8,48,83,10,18,133,61,51,10,241,53,89,73,93,89,14,9,80,197,19,162,11,8,4,9,36,4,14,97,55,12,60,48,8,47,33,118,43,81,8,14,325,9,95,66,36,88,36,48,13,13,4,58,4,166,16,127,4,4,27,11,21,78,24,38,74,8,10,16,7,29,42,4,18,166,124,11,40,49,564,108,9,4,26,68,18,8,62,47,5,34,7,48,348,105,9,18,61,40,183,23,347,81,34,62,78,56,11,212,23,9,55,25,14,72,5,124,282,93,4,21,62,120,3,27,6,60,59,27,88,197,67,9,299,7,9,47,102,229,39,39,7,68,12,60,39,52,138,108,146,15,32,64,76,41,21,169,36,227,37,106,10,27,34,46,8,46,48,89,79,57,9,2,57,29,59,56,9,275,10,14,35,13,52,8,13,67,26,112,8,14,87,7,9,15,11,7,96,53,111,9,4,279,4,13,7,65,11,38,60,29,18,24,14,4,65,102,48,59,4,15,120,15,94,60,237,4,47,110,4,101,7,79,10,9,335,67,9,50,318,149,41,50,8,80,35,18,13,55,28,54,38,23,4,97,55,30,18,22,427,24,59,115,141,300,35,33,93,35,13,40,27,16,176,88,29,10,38,83,112,131,7,19,11,110,23,11,74,15,294,16,29,80,2,4,15,7,52,21,8,20,24,30,8,278,21,58,69,10,177,11,72,41,52,77,14,350,4,9,10,66,46,171,9,21,15,14,16,70,39,66,31,30,11,16,63,70,218,45,11,9,20,44,10,25,37,91,408,214,31,25,228,93,52,5,38,78,9,4,70,4,144,68,10,59,93,90,16,127,54,84,3,27,66,7,87,8,52,54,4,29,84,19,19,113,5,4,4,21,46,42,89,29,159,36,4,128,16,19,43,13,36,32,45,45,133,7,20,74,142,11,56,55,74,28,123,39,7,53,441,11,88,7,25,16,8,43,4,54,49,16,44,93,5,41,112,15,177,66,4,100,31,20,43,10,65,96,186,51,69,75,35,71,62,8,120,7,29,47,128,32,72,103,47,64,18,13,10,91,14,117,11,41,5,92,61,57,17,122,56,32,60,64,356,6,52,4,25,30,8,54,61,13,86,160,56,21,96,83,10,89,14,70,32,4,116,10,16,8,217,62,9,5,232,14,130,28,35,40,43,7,136,24,262,168,40,83,135,37,119,20,16,144,34,4,62,15,50,29,22,16,4,162,114,102,13,12,118,35,21,9,16,32,14,308,19,104,12,60,236,248,91,56,136,64,33,182,212,170,45,53,10,36,72,185,91,47,76,6,5,69,8,72,62,20,74,26,23,51,170,6,34,107,70,13,85,69,34,28,29,78,163,9,7,128,98,13,12,50,146,12,65,2,205,9,184,4,33,34,131,9,12,42,86,38,166,110,75,9,32,22,44,9,77,326,4,40,52,7,27,11,123,34,8,108,8,82,36,151,26,33,4,5,4,14,53,9,160,35,59,281,417,148,11,26,11,219,4,108,22,213,18,171,79,86,8,11,129,11,24,87,4,35,98,3,11,9,56,38,247,45,46,110,61,32,44,8,37,121,6,13,111,6,260,70,8,30,86,31,4,22,48,41,4,39,10,18,104,92,5,71,71,22,50,42,124,5,6,19,11,37,41,38,35,75,43,365,26,40,26,9,29,13,39,45,41,8,9,70,51,13,56,23,143,378,10,10,40,71,17,9,28,36,310,8,18,63,251,9,19,21,201,4,47,44,33,18,16,30,4,67,32,178,39,9,6,56,4,21,312,145,26,16,5,8,30,18,105,13,110,88,15,11,37,36,32,43,6,53,52,27,79,72,6,13,10,103,9,67,17,68,94,82,9,4,71,44,4,36,158,62,31,203,23,41,7,62,11,57,36,69,12,42,78,8,4,18,235,37,38,4,267,26,12,49,13,155,48,4,26,17,71,4,5,43,41,38,84,11,119,13,88,4,47,26,127,227,97,8,23,132,21,8,221,28,7,38,4,4,187,26,264,15,73,3,19,57,193,4,5,50,13,34,42,12,80,15,4,6,274,32,67,28,17,5,47,95,74,22,126,28,72,101,9,112,42,98,162,61,117,61,37,157,30,14,6,4,8,24,9,17,114,174,30,110,46,115,4,14,34,122,52,8,21,12,310,44,75,51,32,49,58,9,9,206,9,37,43,61,112,209,25,61,46,17,10,20,7,11,250,49,23,22,31,15,25,228,21,4,39,44,17,39,106,7,11,24,11,11,44,113,16,25,11,142,22,51,9,11,158,31,137,42,35,73,310,13,47,90,41,81,4,4,10,15,183,9,62,23,334,102,9,96,386,206,35,274,3,4,46,224,73,65,87,30,83,11,63,94,34,118,45,10,8,23,40,12,13,74,19,21,70,67,131,123,10,111,11,25,4,4,60,71,6,45,67,24,43,90,4,57,19,103,30,79,14,164,17,168,45,9,4,42,9,67,75,25,7,49,8,4,140,84,100,13,254,40,31,291,17,30,59,38,69,109,17,73,21,4,5,56,107,8,56,37,122,312,181,92,8,39,169,23,7,16,268,23,10,60,4,18,292,56,13,17,18,67,108,118,63,4,86,9,9,25,92,111,108,36,52,14,46,91,8,131,9,164,4,40,9,12,198,73,2,124,13,68,128,55,180,50,17,42,5,16,48,63,4,115,100,9,5,95,14,36,68,25,71,12,15,19,85,48,7,30,14,107,34,58,23,92,9,59,10,35,29,26,22,4,16,96,68,20,61,321,17,128,52,20,68,49,8,4,33,190,493,14,57,27,36,73,105,30,36,133,12,89,27,70,406,9,4,10,90,4,7,187,47,11,4,81,23,262,4,7,47,36,176,110,416,31,33,165,8,48,29,5,122,38,14,31,4,13,210,212,42,14,53,60,13,54,224,54,66,91,39,74,61,37,53,36,39,63,83,32,136,50,75,55,41,7,14,4,10,8,25,70,21,67,18,9,35,112,10,6,12,54,14,18,320,303,4,16,5,110,215,51,61,5,164,39,23,46,29,195,10,23,90,87,44,257,86,132,83,7,4,108,107,22,124,25,51,11,382,25,59,195,17,33,502,22,9,7,8,27,32,64,13,146,483,29,12,137,219,4,98,131,36,30,24,9,107,237,166,177,4,128,26,4,29,116,9,196,8,42,13,7,7,51,43,33,16,10,45,22,80,183,21,21,80,50,9,6,9,82,141,61,95,26,32,5,10,34,39,127,27,6,23,42,25,37,4,48,38,44,38,206,50,76,4,10,40,36,13,7,28,164,10,57,67,96,4,18,9,13,24,126,285,26,38,17,181,7,37,56,99,82,31,179,27,20,4,83,35,47,33,183,11,53,48,4,20,27,13,116,139,96,33,230,25,10,226,12,20,38,728,71,4,9,4,18,60,38,43,12,90,50,40,136,20,52,41,49,42,61,10,21,74,10,52,60,23,19,57,273,9,55,4,12,57,22,71,44,62,146,9,83,23,9,13,433,141,4,26,90,56,147,59,9,9,4,12,80,5,12,51,4,21,188,305,62,84,163,24,18,16,81,13,17,37,17,176,21,41,35,324,87,10,59,31,17,9,110,148,585,13,23,179,7,50,29,45,40,131,12,4,11,61,221,23,49,42,13,15,101,42,140,53,59,20,65,18,23,123,62,35,30,81,34,146,9,107,7,135,28,9,10,143,127,24,10,88,43,9,41,25,36,42,270,185,18,78,125,94,60,184,47,4,147,29,28,34,4,4,18,15,100,27,68,28,9,196,59,40,9,70,48,7,13,160,43,194,36,26,7,96,17,20,53,262,4,11,339,71,155,16,9,43,82,128,9,89,48,9,11,124,18,94,111,35,382,22,19,29,64,93,15,57,4,377,4,5,133,50,41,6,14,38,154,36,9,448,24,138,366,30,210,23,4,7,30,163,22,8,36,38,7,44,44,11,21,4,23,331,32,48,4,87,169,32,107,26,52,105,11,57,48,88,16,102,109,83,112,52,24,54,27,78,124,41,93,4,110,40,7,366,16,123,4,17,64,8,9,47,12,66,54,52,5,80,26,62,15,80,32,42,53,368,6,110,47,61,92,112,76,164,387,50,122,93,39,245,131,29,77,31,189,20,88,14,25,16,362,9,48,19,167,227,5,30,92,102,41,581,142,64,74,4,87,51,111,29,9,69,63,6,389,363,9,4,156,260,23,50,63,126,11,152,61,60,120,18,54,351,55,107,22,177,23,15,64,4,14,4,324,15,85,131,73,18,178,13,93,14,12,28,33,35,140,77,48,217,20,167,4,9,94,96,49,306,41,112,99,18,3,91,12,38,77,304,183,30,34,53,37,53,236,17,148,163,8,5,9,76,32,21,41,17,23,5,93,17,5,4,34,55,61,120,134,49,2,291,63,413,61,119,130,9,58,36,4,36,59,30,60,4,30,109,109,67,120,110,39,36,37,4,23,5,9,159,22,185,10,35,54,46,93,30,23,4,84,61,85,27,59,9,25,4,27,39,40,79,38,49,4,136,76,12,31,174,25,27,95,10,17,11,92,122,12,11,88,20,89,4,11,56,7,14,18,7,32,66,282,102,4,254,26,13,8,14,67,40,50,107,12,9,115,22,24,4,12,49,6,42,48,35,191,150,9,8,18,16,165,196,9,4,85,8,65,20,6,41,17,7,40,157,35,43,10,28,4,4,13,53,227,49,25,62,17,102,31,129,9,3,86,36,36,55,125,26,21,85,34,11,33,57,33,53,63,22,9,143,21,139,87,3,10,91,5,7,22,9,82,8,314,4,48,12,8,49,26,59,8,68,149,14,43,14,62,4,8,19,376,9,13,55,104,4,61,36,127,30,36,4,25,42,39,17,48,26,41,61,53,38,27,254,8,12,86,9,9,14,21,9,8,113,4,7,92,14,39,120,63,315,52,63,46,27,4,209,35,89,128,41,8,7,7,77,13,147,39,42,4,48,5,320,234,45,6,22,110,120,362,134,77,17,28,11,242,54,4,10,29,4,11,14,79,19,77,169,35,150,12,6,81,26,52,24,11,5,209,59,20,30,14,31,86,21,107,22,34,24,66,97,5,73,27,8,4,3,34,48,57,109,29,36,113,36,41,138,15,87,48,27,57,65,9,72,59,21,47,191,153,13,12,38,82,16,17,121,48,165,20,22,9,58,73,7,4,134,9,33,186,174,92,4,45,64,83,51,327,52,9,60,14,61,108,167,34,5,13,112,33,13,42,12,112,49,123,53,19,95,13,45,43,12,60,120,369,43,118,40,94,31,21,63,24,112,16,43,9,144,30,8,36,116,27,73,13,44,88,92,9,13,27,95,37,70,159,107,89,37,126,54,64,27,34,14,15,4,383,141,55,27,117,128,14,10,46,63,54,18,4,4,66,100,216,83,7,10,9,32,53,70,34,56,22,5,23,31,59,18,8,15,76,19,88,27,54,38,9,9,166,9,11,57,46,24,17,14,27,18,37,170,127,32,38,87,4,4,12,9,109,66,28,4,16,217,83,261,32,96,57,4,21,10,9,45,34,8,179,32,102,13,7,13,26,8,14,17,13,59,72,12,113,20,12,2,9,116,30,77,139,14,115,12,64,72,97,107,14,178,68,4,283,10,21,9,7,21,4,4,13,57,25,381,172,20,61,38,78,9,9,8,11,50,4,37,217,8,20,31,72,29,197,155,9,181,5,8,346,28,25,90,125,9,4,32,5,5,221,119,15,31,13,11,28,7,4,28,4,92,85,51,31,23,146,12,16,96,4,58,14,148,19,91,26,65,24,119,42,74,129,134,31,37,152,70,65,48,59,4,12,5,13,36,9,35,19,341,42,179,5,10,405,26,83,4,35,48,141,86,60,67,40,68,11,4,69,126,62,5,195,43,28,30,4,105,25,25,71,78,79,17,194,298,20,23,129,62,12,9,48,56,12,186,30,45,16,31,110,14,79,33,34,204,9,55,44,28,70,49,175,41,5,34,65,53,44,169,23,80,4,60,11,129,15,43,107,5,50,93,25,68,63,43,16,24,48,9,10,9,49,18,58,4,48,44,26,147,70,46,165,62,68,53,92,12,18,21,40,39,25,28,66,4,35,20,42,42,4,16,95,56,9,20,9,34,32,41,15,10,28,327,56,34,72,408,30,33,4,8,69,140,265,19,71,92,197,16,15,36,57,62,26,33,17,170,26,85,163,42,12,139,13,8,64,19,99,66,80,163,15,36,17,37,17,112,27,347,27,9,30,10,8,46,39,37,301,439,167,9,117,5,130,13,123,323,9,9,22,55,53,9,50,9,16,22,298,99,28,46,18,17,21,13,131,25,4,25,11,73,31,117,12,37,4,10,9,17,400,32,71,46,30,7,201,9,180,60,131,42,304,7,29,215,35,44,80,74,67,4,4,31,278,11,34,111,4,67,76,50,54,9,4,28,14,16,44,8,9,20,5,29,178,96,16,34,88,77,121,95,27,33,72,23,20,106,20,31,95,74,60,15,44,97,221,77,45,6,64,58,7,243,16,217,23,28,4,171,116,38,63,52,10,91,32,73,48,97,82,63,4,7,16,33,7,169,40,54,9,16,31,13,4,39,30,175,132,17,159,43,41,8,55,8,13,26,10,58,58,10,121,275,33,115,36,71,25,7,128,48,149,34,19,2,283,9,44,334,30,32,8,193,55,185,20,81,17,207,41,94,18,76,4,35,43,91,13,208,61,44,137,34,4,7,45,5,4,8,96,444,355,47,48,279,118,64,37,83,29,80,18,85,4,12,22,14,4,20,4,50,66,6,5,36,81,50,2,13,59,47,65,36,107,90,7,7,135,7,2,15,83,8,43,36,8,256,20,16,29,55,132,9,76,13,76,21,47,73,55,39,7,163,38,15,12,73,71,68,35,8,71,12,22,32,41,53,81,147,25,14,7,40,389,16,11,39,24,39,107,37,16,4,105,28,29,9,39,146,174,6,114,9,112,68,134,44,155,332,62,11,35,71,17,262,106,8,247,137,95,27,19,7,85,172,13,26,13,203,40,45,133,371,37,10,12,147,106,183,17,7,9,16,39,19,52,27,28,80,69,19,10,20,122,99,66,142,14,68,39,108,17,40,10,47,6,102,8,2,62,27,241,43,8,28,11,4,4,17,4,33,383,11,354,70,10,26,25,32,11,20,37,8,312,56,149,9,13,133,94,35,60,4,54,29,28,35,17,9,12,56,27,4,103,237,10,4,64,64,14,13,21,68,76,10,27,218,90,7,98,244,21,66,25,7,4,11,9,6,8,4,52,295,17,9,57,9,84,12,3,9,17,3,9,65,24,127,47,40,15,67,7,42,55,25,13,9,72,41,9,42,5,57,18,75,217,8,60,48,61,23,13,9,29,63,12,39,42,50,43,59,104,85,58,32,29,110,33,55,14,151,37,28,119,72,45,258,17,63,28,29,97,21,262,185,4,142,55,91,48,20,8,21,47,8,29,7,81,13,121,139,97,12,52,6,4,11,37,18,4,72,10,98,74,56,3,16,15,96,73,43,39,65,15,49,183,8,89,11,16,102,106,77,234,42,34,32,17,276,8,52,6,13,54,8,7,182,7,59,25,5,11,72,69,24,13,161,255,232,67,4,9,61,42,65,25,105,87,21,22,57,64,36,30,17,9,12,95,6,13,4,53,9,52,119,117,5,8,211,29,69,33,4,71,13,10,76,4,30,60,27,3,48,28,44,31,140,16,52,30,18,4,16,4,4,30,71,25,334,84,9,51,9,9,334,105,9,27,82,53,56,11,7,17,4,51,323,26,9,7,74,7,28,239,139,57,4,23,11,12,13,9,82,25,48,9,57,255,80,45,4,46,47,49,51,100,12,74,9,81,18,63,87,67,177,738,28,426,7,6,8,4,94,46,7,42,97,4,24,7,165,108,7,45,17,147,92,11,419,165,13,62,86,154,124,20,47,43,27,133,40,86,16,73,386,15,15,10,4,4,8,40,206,4,78,37,49,9,51,4,79,28,4,28,21,69,52,20,27,102,80,9,168,9,11,124,30,17,9,55,38,40,98,86,121,131,34,40,18,16,8,70,11,28,28,15,7,8,36,75,37,47,12,15,23,134,19,9,19,23,47,14,96,42,16,224,5,128,34,53,4,19,15,70,47,92,15,54,55,20,24,8,29,188,76,122,306,9,21,59,28,4,29,174,20,8,20,10,113,41,35,71,49,4,188,231,4,67,21,52,7,175,12,94,13,178,10,20,40,34,4,10,94,15,139,28,193,337,4,38,237,37,5,5,160,66,52,105,9,158,182,8,282,29,12,240,121,50,91,73,46,39,17,9,152,117,33,5,11,17,90,26,78,179,162,22,9,224,48,72,342,8,4,56,10,91,26,29,37,26,47,85,39,280,116,7,58,14,6,46,28,375,18,59,11,40,96,7,33,17,28,143,62,64,17,4,9,10,85,40,55,18,38,44,17,141,4,110,39,194,20,31,105,32,9,88,37,9,22,77,347,28,195,5,66,31,16,23,37,39,21,12,10,4,20,15,53,12,35,9,9,52,23,75,109,187,228,183,4,262,27,50,61,42,54,72,44,69,23,76,8,8,25,73,63,61,163,14,34,35,31,48,10,63,13,29,4,296,84,18,134,62,4,64,54,274,55,31,199,74,121,12,66,265,17,29,27,83,4,9,172,17,93,48,55,14,8,28,27,107,84,15,4,119,7,26,24,5,72,112,85,58,50,52,107,18,9,18,11,37,4,73,31,47,71,147,18,93,7,4,13,98,83,64,29,52,16,10,87,52,55,13,29,20,21,8,110,9,25,9,35,35,9,144,148,84,173,7,181,28,34,217,184,221,13,18,20,16,11,13,8,59,64,7,137,71,101,9,16,18,68,41,137,6,4,5,11,4,75,18,213,90,6,74,14,132,9,117,8,66,4,43,61,9,314,359,18,30,62,165,21,4,130,11,20,23,30,96,7,221,73,268,107,29,260,41,4,33,28,14,46,9,66,67,12,66,113,12,42,85,51,45,9,5,181,20,34,39,16,37,9,32,148,9,59,25,13,130,4,68,7,84,181,68,42,42,4,122,450,28,33,187,137,332,7,56,52,36,100,8,22,35,4,12,50,163,19,66,133,185,46,42,8,44,74,8,10,7,50,189,24,102,28,59,9,74,196,267,20,10,9,54,49,28,135,82,4,28,35,46,82,195,208,49,24,42,25,2,16,20,40,20,96,28,19,18,50,235,13,30,291,47,63,220,45,96,74,109,99,252,42,69,15,28,4,35,103,72,47,34,17,38,8,3,4,22,57,59,39,13,23,57,258,64,43,190,13,54,56,94,66,15,105,80,130,14,42,7,46,39,276,31,8,379,52,4,206,15,108,13,4,28,195,37,14,85,49,62,33,99,58,24,145,27,22,16,33,19,30,20,46,72,40,119,175,54,23,98,41,145,68,19,136,9,331,12,3,43,80,15,74,32,55,396,21,13,44,9,4,4,105,8,8,6,380,41,4,7,37,129,71,33,20,40,52,4,9,8,4,153,12,31,67,12,176,8,18,18,92,81,15,4,40,60,19,9,40,9,111,35,8,191,66,15,51,11,112,8,11,29,30,38,121,112,312,74,7,141,67,42,29,10,10,68,9,23,57,3,78,9,8,88,26,18,242,18,3,79,17,66,4,12,105,71,21,111,37,194,42,93,5,16,11,99,50,13,9,36,23,119,31,4,96,45,99,6,11,50,20,80,6,9,4,59,59,37,38,82,35,55,162,103,41,44,3,4,18,38,51,137,5,43,12,37,9,56,16,64,15,62,140,9,33,35,31,161,38,8,131,5,26,77,4,4,27,18,18,94,31,50,65,132,31,96,19,127,88,29,11,75,8,8,5,21,21,95,60,39,4,176,53,130,32,188,18,52,12,5,7,74,11,49,37,69,101,80,88,9,136,8,85,25,429,11,178,4,95,156,15,49,21,134,22,29,25,12,60,92,31,13,17,20,18,31,32,77,11,7,45,135,102,93,17,97,168,6,17,31,83,125,59,218,73,9,8,47,313,399,59,6,50,4,4,13,126,22,64,4,46,161,10,31,7,159,23,7,7,45,17,165,15,19,98,85,87,7,9,116,121,6,90,30,18,50,54,40,39,4,157,69,10,16,83,4,24,18,66,5,163,104,31,230,9,30,176,8,115,20,41,20,49,130,49,2,67,20,4,67,6,54,9,120,24,75,213,4,101,67,372,40,7,14,63,124,8,45,17,24,13,8,159,237,39,58,139,34,72,38,9,306,177,27,19,13,97,44,4,7,12,45,31,88,53,45,11,31,4,14,4,52,58,16,95,20,33,40,75,18,75,42,237,71,31,79,9,68,4,5,154,39,15,81,4,7,16,238,46,178,40,5,75,42,8,10,177,52,82,213,11,28,36,144,48,23,42,48,84,76,49,163,34,49,21,23,111,23,37,5,5,40,186,66,139,75,89,46,13,114,107,28,141,215,62,225,74,13,22,38,11,43,6,96,177,25,307,99,14,21,34,281,10,53,20,201,97,69,73,297,32,8,8,27,44,156,53,4,7,54,19,16,23,16,416,16,32,4,35,7,49,42,160,27,124,16,30,9,46,71,19,84,36,39,75,68,96,12,43,52,6,15,73,49,18,13,6,39,4,9,14,197,14,145,30,23,77,83,5,337,8,24,23,132,107,75,16,39,12,49,63,58,57,81,152,132,51,30,45,7,50,8,75,104,199,47,51,13,54,74,92,206,70,8,34,45,60,52,54,154,106,5,43,9,8,19,72,21,9,19,94,55,55,179,72,20,4,7,11,26,9,24,11,103,4,5,12,150,18,124,399,13,16,35,13,8,7,45,70,13,13,9,7,334,15,127,61,29,58,69,54,30,6,20,129,11,87,38,24,10,13,28,53,10,7,5,36,88,37,133,46,154,18,9,77,348,8,7,7,20,8,99,75,388,70,25,12,5,104,24,5,11,29,10,87,168,61,89,21,24,9,87,198,22,24,102,169,4,76,80,12,88,303,50,9,4,6,26,4,5,294,64,84,47,48,12,9,77,106,333,110,23,20,4,4,8,253,24,36,12,4,37,16,34,10,26,204,165,50,14,228,4,29,5,238,45,31,54,86,27,46,33,20,53,27,15,80,9,23,79,21,60,433,4,4,26,50,49,219,39,21,26,36,8,89,17,119,115,4,97,49,54,47,2,55,103,39,20,31,93,4,79,279,34,60,23,9,66,76,34,119,17,11,32,40,37,7,80,11,35,58,73,47,32,9,17,17,20,146,27,12,43,41,80,219,27,24,182,38,15,144,76,13,25,126,18,79,233,5,4,46,174,119,49,14,7,4,58,18,13,11,12,23,76,8,10,16,50,40,8,29,9,29,15,120,10,4,27,15,34,16,7,42,20,112,132,74,88,273,7,26,59,75,17,30,16,110,25,58,14,4,4,31,70,201,36,32,247,115,10,38,159,27,44,36,26,13,60,13,38,30,28,9,4,92,2,21,51,34,11,5,73,73,43,169,33,47,60,43,30,11,54,31,6,79,4,13,30,13,6,117,29,45,4,57,6,8,20,165,140,30,32,8,14,59,90,36,16,61,105,60,17,38,48,59,15,4,9,4,74,133,247,40,9,134,27,13,31,28,9,13,13,8,21,55,77,110,4,60,22,9,54,16,426,34,18,75,146,180,6,13,34,124,19,9,8,9,35,52,72,19,153,6,74,29,80,22,234,37,44,13,15,21,4,3,27,66,4,108,8,9,8,70,149,137,58,19,7,178,53,40,114,25,52,279,38,109,74,30,51,5,31,14,18,24,9,55,189,25,9,157,61,26,16,116,39,59,15,79,81,81,9,90,136,33,44,13,22,30,25,32,13,37,2,338,20,37,18,30,77,11,24,116,6,12,206,85,93,79,152,130,80,71,50,20,249,2,41,51,72,32,13,88,32,57,15,9,5,53,317,153,41,32,43,29,9,50,12,84,271,51,17,64,53,7,81,58,56,4,344,16,44,50,49,25,46,82,77,23,115,47,244,13,49,202,69,29,2,162,168,52,17,8,139,63,11,80,16,29,1518,176,35,11,24,23,197,62,51,107,2,48,12,46,10,166,38,11,120,8,154,43,66,51,53,27,33,16,78,14,20,13,42,7,33,25,149,115,153,4,97,60,65,124,8,29,42,56,8,147,104,5,194,53,38,13,43,121,37,21,40,202,181,14,21,26,137,93,8,52,4,43,110,6,151,12,8,124,71,157,67,125,13,167,54,21,21,7,110,22,78,50,7,8,13,56,84,9,96,59,130,15,9,37,11,16,176,23,18,79,34,12,37,196,57,30,76,7,90,30,38,197,120,16,111,4,352,75,85,57,56,48,9,47,54,26,15,46,70,50,34,9,216,4,9,7,43,10,107,4,24,17,10,11,6,111,405,9,50,84,8,9,42,10,19,14,11,108,4,186,228,156,60,7,120,25,11,39,83,42,66,13,4,51,13,7,35,79,10,148,122,13,17,34,55,78,11,12,16,4,13,26,38,50,98,239,30,198,21,46,47,194,53,38,115,251,220,6,23,312,197,20,9,54,41,57,27,286,8,34,4,79,17,55,67,129,227,13,30,296,42,224,64,4,13,9,87,28,11,5,38,29,39,30,8,10,92,38,38,42,27,128,117,5,50,16,92,56,139,34,20,48,9,15,19,219,67,150,11,64,4,117,29,14,15,8,199,18,22,37,10,183,38,72,4,230,177,170,55,112,24,58,11,26,13,7,80,15,29,20,14,39,15,9,32,14,19,60,5,8,156,26,58,4,13,4,38,9,7,170,365,270,9,5,139,9,8,128,62,26,25,5,11,47,140,18,14,33,52,99,162,61,8,9,98,59,8,76,28,7,5,149,270,44,116,204,114,52,40,55,13,32,36,14,107,33,282,189,9,35,112,107,10,15,153,165,20,36,45,4,4,89,25,14,8,53,89,4,12,8,50,13,9,9,57,185,4,8,9,30,72,4,34,27,110,124,52,42,9,55,5,127,9,17,144,10,5,116,161,299,106,248,49,4,33,76,37,4,142,40,42,101,20,28,234,18,142,8,5,4,9,69,147,22,56,342,134,33,2,49,21,12,29,58,88,35,49,74,19,86,49,270,11,68,10,5,78,5,26,19,53,19,8,13,54,9,17,70,48,50,7,132,131,40,36,22,4,51,39,52,20,4,8,4,4,43,24,11,58,70,4,10,350,94,39,13,154,22,23,16,51,16,29,8,4,54,20,7,5,3,38,31,252,52,26,48,16,54,54,50,17,15,117,8,15,28,55,46,19,61,47,76,58,98,278,140,8,38,332,74,18,91,23,15,26,35,8,12,54,4,4,37,7,9,400,34,8,42,71,24,58,40,71,98,9,37,101,153,260,54,122,25,61,9,76,160,9,34,109,12,4,109,103,52,94,98,62,83,19,26,17,33,4,22,106,9,54,62,50,94,116,100,117,73,25,11,19,79,3,102,12,182,40,13,47,4,24,75,17,37,113,59,121,4,33,7,70,8,67,49,81,83,49,16,5,142,21,298,109,152,43,5,11,42,89,165,46,29,42,131,46,40,65,7,293,181,42,146,31,6,25,33,54,38,19,125,49,18,60,18,165,85,145,138,12,21,11,224,95,85,44,12,349,13,146,57,9,67,27,4,67,41,91,9,13,8,21,69,4,46,28,4,8,33,3,46,17,243,30,4,45,44,53,16,9,14,46,70,4,95,8,10,75,16,23,61,117,8,146,80,158,4,141,14,16,30,30,318,53,163,228,17,13,49,32,29,104,10,18,10,63,123,251,48,32,79,82,93,86,9,46,9,115,4,19,15,27,7,24,8,178,10,42,15,26,31,29,129,17,354,7,13,86,155,116,370,39,35,135,127,19,46,35,13,4,55,25,4,16,158,198,13,96,8,13,260,15,68,60,38,146,37,99,25,58,5,28,81,39,12,22,11,54,34,267,13,26,38,88,69,115,51,156,70,5,12,76,7,85,88,42,24,416,36,15,55,75,93,9,5,13,17,51,64,28,35,7,78,8,40,17,147,172,68,135,17,248,28,70,244,8,7,310,54,8,80,46,117,23,46,47,4,58,125,62,230,12,35,113,100,32,178,9,10,9,33,28,4,110,81,195,132,126,27,122,121,25,67,99,222,59,180,18,4,30,31,76,4,76,29,330,47,119,55,208,149,75,16,9,167,4,70,11,4,7,237,13,5,83,43,283,70,4,76,188,179,434,30,21,105,9,6,36,20,107,102,49,285,234,15,11,29,135,9,63,93,19,23,50,13,44,21,36,27,8,26,37,34,10,29,30,4,9,16,71,147,8,426,205,20,226,24,5,44,236,19,40,7,170,9,106,170,73,41,37,33,138,7,95,47,39,51,9,9,7,34,10,59,18,21,43,98,39,12,160,78,89,34,59,218,54,8,18,55,18,86,30,35,4,70,35,45,9,26,31,10,39,19,68,20,17,167,8,37,9,54,65,104,13,32,55,56,8,6,236,61,17,7,4,4,8,5,68,99,11,33,58,52,158,22,8,35,57,60,9,29,34,12,38,7,22,26,14,35,70,6,46,117,12,87,78,26,147,202,74,77,101,56,419,4,69,72,8,4,30,10,23,41,9,50,29,53,102,5,31,10,4,12,5,21,279,54,180,46,26,41,27,5,328,67,27,77,51,10,34,11,29,35,43,78,143,36,57,77,11,26,31,14,71,36,51,196,25,5,48,34,7,18,6,46,4,141,74,23,175,25,122,90,30,125,26,8,9,29,45,427,124,15,15,64,14,81,26,51,4,29,4,4,237,14,59,12,7,12,142,162,19,40,19,6,335,22,9,44,26,14,19,8,19,403,140,8,35,4,29,24,9,41,34,202,119,128,51,18,4,51,28,94,19,42,51,4,129,7,63,20,68,256,55,39,31,7,81,50,22,127,203,37,41,79,90,43,70,102,69,90,152,100,32,54,160,10,7,8,29,28,24,127,9,285,13,20,16,2,74,93,21,9,7,100,57,42,61,66,40,48,137,187,4,4,42,40,55,22,51,156,52,23,243,111,253,51,436,4,12,146,66,112,48,82,188,92,8,291,15,147,3,17,9,125,58,29,78,49,28,103,31,9,12,153,28,9,17,52,49,92,111,8,8,4,7,49,8,2,8,53,27,37,37,4,89,9,9,203,71,27,200,132,154,54,36,28,50,46,37,171,158,21,7,160,94,11,104,4,83,16,154,103,87,38,104,14,76,4,27,57,35,31,78,8,14,8,13,44,48,8,4,5,77,16,28,36,4,26,83,107,10,53,64,4,115,15,4,17,23,9,90,379,229,26,20,65,64,8,93,48,29,26,148,4,37,244,7,33,41,27,126,425,60,7,4,136,5,40,11,71,84,58,92,8,9,64,6,19,135,15,64,181,94,24,9,4,35,11,18,7,48,27,46,405,30,25,156,9,23,90,154,5,18,98,45,10,73,49,6,51,9,11,11,194,11,160,233,18,45,39,10,61,162,77,129,18,349,21,52,10,172,9,22,11,13,90,7,9,21,4,47,9,6,16,68,9,30,4,17,33,50,93,152,57,20,2,4,89,45,409,70,84,4,45,30,23,79,92,62,2,4,233,4,51,104,6,35,36,9,64,18,12,30,13,38,31,14,46,383,168,19,28,65,8,7,11,48,104,216,9,9,33,90,83,22,40,4,95,33,29,15,183,13,12,21,5,68,52,47,21,209,364,51,24,4,128,12,98,35,84,55,81,7,93,34,26,21,13,179,645,54,9,118,5,36,95,4,32,31,32,107,269,25,5,4,72,92,43,54,403,55,7,20,5,102,13,12,16,55,53,47,76,43,16,27,106,28,110,56,41,10,18,85,10,8,114,6,22,43,20,53,19,60,29,31,101,215,3,23,15,45,21,25,211,153,40,4,252,45,7,197,9,30,32,48,49,191,73,78,343,263,17,100,39,322,23,195,71,31,17,62,133,7,21,431,199,26,72,4,42,23,12,46,10,13,8,18,95,76,337,5,31,39,15,29,266,31,32,8,9,94,17,56,98,36,5,22,11,44,29,414,101,77,72,320,13,7,7,83,42,175,22,38,44,159,374,32,7,9,17,57,40,30,64,76,21,47,8,111,39,42,53,273,89,12,39,94,5,50,250,40,37,99,149,14,110,4,46,83,41,55,72,39,56,172,11,154,4,13,24,16,4,70,35,206,18,40,41,16,174,32,52,69,186,44,12,157,13,4,73,52,35,86,22,68,48,68,43,83,10,17,15,69,13,15,7,9,172,39,20,25,323,15,7,95,18,130,227,200,97,43,9,12,4,12,67,4,38,99,4,288,17,49,81,51,102,9,92,21,4,9,16,33,26,85,7,18,30,24,19,118,215,86,11,13,7,369,6,4,12,42,149,31,13,3,7,9,108,11,33,71,136,4,15,27,39,257,179,11,43,64,155,110,11,7,15,54,185,17,56,55,38,21,61,27,9,13,33,10,26,8,164,52,14,46,384,66,11,4,22,4,35,24,37,106,22,224,50,104,33,82,93,15,98,41,150,52,77,20,38,16,3,16,42,46,16,189,40,76,20,71,9,20,9,63,8,48,60,71,57,9,162,104,89,13,196,27,128,110,13,4,48,183,69,279,4,46,139,46,37,112,9,50,78,30,25,8,4,35,46,30,76,76,16,25,9,9,106,88,9,22,7,10,89,37,23,33,133,113,202,53,134,9,9,4,14,75,24,17,30,335,149,4,5,431,49,9,23,9,52,7,4,14,45,4,25,65,78,130,97,8,4,36,24,7,64,4,51,129,4,131,100,14,65,73,56,14,69,56,9,9,31,13,86,4,19,4,93,42,204,46,113,31,23,104,147,113,126,28,92,28,66,47,7,43,69,14,50,34,222,213,41,5,31,46,104,5,135,68,86,11,36,23,55,77,29,56,44,134,21,23,4,48,75,118,4,9,6,78,34,4,242,34,82,90,310,15,6,39,8,10,13,12,30,46,170,48,5,138,47,24,30,33,77,364,48,18,20,99,57,31,16,50,15,40,37,112,39,50,20,129,4,8,12,80,40,22,29,281,95,4,34,4,88,36,4,164,15,73,24,73,4,7,37,24,81,17,5,60,200,13,167,394,121,261,39,70,42,389,73,9,30,16,23,89,96,44,19,4,197,6,91,8,4,33,76,120,13,225,4,14,9,12,53,43,5,20,55,4,30,64,6,45,144,110,28,159,56,19,62,109,85,20,57,49,40,35,45,54,4,4,56,17,4,15,4,95,22,13,41,238,12,142,117,13,168,40,11,94,106,4,142,42,34,7,17,9,7,71,21,9,4,32,7,232,38,5,7,164,186,34,39,385,230,50,13,72,14,7,60,8,87,7,105,16,16,9,72,15,8,65,112,5,8,29,128,4,5,4,11,360,53,123,177,73,18,10,33,388,48,27,32,39,55,84,19,15,4,20,25,27,20,6,385,70,120,30,81,173,43,66,11,28,9,51,5,8,36,52,32,4,58,232,6,55,24,161,36,32,9,18,104,144,16,164,24,133,22,550,37,32,75,93,32,8,18,4,11,88,5,4,9,9,30,66,434,31,7,7,56,15,29,82,5,37,23,69,10,42,19,7,92,4,65,9,8,36,8,6,24,16,76,8,7,7,4,112,29,47,4,9,109,184,9,100,57,68,61,23,146,236,4,15,67,14,214,28,40,64,13,39,4,420,47,8,19,68,87,37,7,12,32,62,220,16,53,97,127,111,55,15,55,14,18,90,41,33,169,9,164,98,71,65,65,34,16,180,22,23,4,133,65,2,89,28,14,4,4,28,25,4,191,19,25,63,12,61,119,80,7,15,193,56,26,47,18,214,60,99,30,27,7,322,52,8,28,9,7,52,7,80,102,28,48,7,133,107,56,5,331,74,84,16,49,4,7,8,4,13,4,49,110,72,4,103,33,14,13,79,4,4,67,54,9,151,341,4,11,4,120,23,19,7,33,126,60,321,87,106,54,66,1,7,20,4,12,115,60,56,35,34,49,11,16,33,46,4,82,155,36,102,8,8,38,43,13,8,76,141,89,4,16,32,10,63,14,88,122,29,17,8,52,13,4,100,384,4,106,2,4,10,65,79,11,116,2,30,61,26,88,4,116,52,9,7,10,37,10,51,3,322,16,181,77,20,198,169,54,38,4,262,22,10,4,50,102,27,11,118,8,66,34,31,56,55,44,74,101,20,83,4,95,42,29,4,105,85,5,45,229,200,10,154,210,9,28,32,103,99,8,15,102,137,32,8,8,20,8,304,33,55,32,35,210,46,91,11,2,182,102,28,8,27,10,25,95,48,145,4,112,8,337,9,174,44,4,12,177,88,31,130,81,57,7,59,22,14,349,8,74,7,6,20,16,184,27,85,96,9,25,53,32,32,44,48,187,30,35,9,56,9,51,54,4,38,30,14,285,70,158,30,14,19,107,123,135,69,12,18,49,118,44,101,152,104,34,146,37,53,33,22,23,10,33,110,340,78,3,220,168,84,32,9,4,9,8,42,70,22,47,58,187,103,149,5,4,18,4,9,12,9,152,198,19,126,14,35,136,182,112,16,49,91,8,14,47,37,33,21,379,6,17,336,37,37,127,177,28,5,87,40,9,183,14,4,453,67,163,26,13,150,4,2,18,21,19,8,21,64,136,90,7,12,13,4,95,18,7,30,109,333,109,29,38,35,37,9,14,39,23,51,107,54,13,35,282,38,12,9,9,6,22,51,21,9,149,15,40,8,54,140,12,121,46,32,4,155,14,27,58,32,177,9,72,40,16,73,58,45,50,146,37,89,12,44,54,33,9,20,75,23,4,236,14,4,61,9,17,15,49,39,35,4,77,24,8,11,30,89,255,14,82,74,9,39,86,11,4,61,387,27,36,27,25,120,11,205,230,85,28,13,16,53,5,188,42,5,4,12,181,171,4,129,14,101,44,11,21,40,186,12,146,32,25,128,68,8,40,186,32,394,78,119,4,15,34,87,164,163,58,230,91,26,285,4,17,127,3,27,22,4,77,37,112,88,27,4,124,64,11,4,8,286,7,45,130,132,174,10,34,159,9,18,5,13,20,107,44,60,40,2,250,40,84,265,98,23,9,41,21,4,133,153,53,20,8,92,23,57,57,160,372,4,33,425,6,9,32,42,34,11,5,12,268,86,69,4,29,13,13,9,33,20,57,136,135,55,68,102,89,220,130,28,108,16,10,19,20,47,12,79,36,90,64,44,48,21,77,5,55,26,9,9,33,49,43,44,14,15,108,21,72,226,35,132,11,28,67,56,40,7,17,22,6,60,56,21,38,4,43,21,138,10,31,48,20,10,63,13,10,22,76,3,75,153,131,13,33,3,98,79,43,59,119,36,29,142,9,5,43,10,14,5,20,11,43,61,5,60,101,29,58,217,11,9,74,21,4,39,81,44,24,43,224,25,39,12,10,10,79,56,25,54,8,74,112,37,9,4,28,30,30,4,202,13,30,39,5,140,9,16],\"xaxis\":\"x\",\"yaxis\":\"y\",\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"comment_len\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"count\"},\"type\":\"log\"},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Histogram of len of comments\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('59b45b98-4530-478a-a72b-4acd25f5b33d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Čištění textů"
      ],
      "metadata": {
        "id": "dDzHiky-vvjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Make text lowercase, remove text in square brackets, remove links\n",
        "    and remove stop words containing numbers\"\"\"\n",
        "    text = text.lower()                                            # Converts the text to lowercase using regex \n",
        "    text = re.sub(r\"\\[.*?\\]\",\"\",text)                              # Replace's the text into 'nothing\" if text is present inside squre brackets.\n",
        "    text = re.sub(\"https?://\\S+|www\\.\\S+\",\"\",text)                 # Removes the links from the comments.\n",
        "    text = re.sub(\"<.*?>+\",\"\",text)                                # Remove unwanted\n",
        "    text = re.sub(\"\\n\",\"\",text)                                    # Remove next line symbols '\\n'\n",
        "    text = re.sub(\"\\w*\\d\\w*\",\"\",text)                              # Takes only albhabet and digits.\n",
        "    return text\n",
        "train['comment_text_clean'] = train['comment_text_clean'].apply(lambda text : clean_text(text))\n",
        "test['comment_text_clean'] = test['comment_text_clean'].apply(lambda text : clean_text(text))"
      ],
      "metadata": {
        "id": "J5JO9SF-vzmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Odstranění stop slov\n",
        "Stop slova jsou často používaná slova, která nepřidávají do textu žádné další informace. Slova jako \"the\", \"is\", \"a\" nemají žádnou hodnotu a pouze přidávají do dat šum."
      ],
      "metadata": {
        "id": "m-C3Bmgjv2Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "more_stopwords = ['u', 'im', 'c']\n",
        "stop_words = stop_words + more_stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
        "    return text\n",
        "    \n",
        "train['comment_text_clean'] = train['comment_text_clean'].apply(remove_stopwords)\n",
        "test['comment_text_clean'] = test['comment_text_clean'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "0KVWW_zMv2c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming\n",
        "Wikipedie:  \n",
        "\"[Stematizace](https://cs.wikipedia.org/wiki/Stematizace) (anglicky stemming) je nalezení kmene slova (nepřesně a úžeji taky kořene). Algoritmus se nazývá stemmer. V praxi se stematizace používá například ve vyhledávačích, kde dovoluje vyhledávat bez ohledu na konkrétní tvar. Podobnou operaci lematizace provádí lemmatizátor – tato operace vrací základní tvar slova (tj. lemma).\""
      ],
      "metadata": {
        "id": "lLIiS8tGv4qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming the texts\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "\n",
        "def stemm_text(text):\n",
        "    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
        "    return text\n",
        "train['comment_text_clean'] = train['comment_text_clean'].apply(stemm_text)\n",
        "test['comment_text_clean'] = test['comment_text_clean'].apply(stemm_text)"
      ],
      "metadata": {
        "id": "S06Zwjziv2k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_classes = ['category_CONTENT', 'category_INTERFACE', 'category_SUBSCRIPTION', 'category_USER_EXPERIENCE']\n",
        "target_data = train[list_classes]\n",
        "train_data = train['comment_text_clean']"
      ],
      "metadata": {
        "id": "yaEkQfRKv9Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Fine-tuning a pre-trained BERT model**"
      ],
      "metadata": {
        "id": "GDgwkXxuGna1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. It was originally published by\n",
        "\n",
        "- Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova: [\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"](https://arxiv.org/abs/1810.04805), 2018.\n",
        "\n",
        "This TF Hub model uses the implementation of BERT from the TensorFlow Models repository on [GitHub](https://github.com/tensorflow/models/tree/master/official/legacy/bert).  \n",
        "BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters"
      ],
      "metadata": {
        "id": "w_d84Pjj4u-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####První krok:  \n",
        "Definujeme url pro enkodér BERT a url pro model předzpracování, které se načítají z hubu TensorFlow.  \n",
        "   \n",
        "\n",
        "*   Do proměnné \"encoder\" umístíme *předtrénovaný model BERT*.\n",
        "*   *Model předzpracování* slouží k přípravě vstupního textu tak, aby mohl být vložen do enkodéru BERT. Zahrnuje tokenizaci, mapování tokenů na jejich ID a přidání speciálních tokenů [CLS] a [SEP] na vstup. Tyto speciální tokeny hrají roli v mechanismu pozornosti, který BERT používá.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gc8R8-GiFpBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BERT encoder url and processing url blocks from tensorflowhub\n",
        "encoder = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\" \n",
        "preprocessing = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
      ],
      "metadata": {
        "id": "OvuyRhrAv-oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Druhý krok: \n",
        "Vytvoříme instance KerasLayer pro model předzpracování i enkodér BERT, které byly načteny v prvním kroku. To umožňuje snadné použití modelů v modelové architektuře Keras.\n",
        "\n",
        "Tento krok je nutný, protože model předzpracování a enkodér BERT, které byly načteny v prvním kroku, nejsou nativními modely Keras. Jedná se o moduly TensorFlow Hub, což jsou předtrénované modely, které lze načíst a použít v TensorFlow. Nemají však formát, který by bylo možné použít přímo v architektuře modelu Keras.\n",
        "\n",
        "Abychom mohli tyto moduly TensorFlow Hub použít v modelu Keras, musíme vytvořit instance KerasLayer class pro model předzpracování a enkodér BERT. Class KerasLayer je obalová metoda, kterou poskytuje TensorFlow Hub a která umožňuje používat moduly TensorFlow Hub jako modely v architektuře Keras."
      ],
      "metadata": {
        "id": "ZZaLXncrJG2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocessing_model = hub.KerasLayer(preprocessing) # preprocessing step in bert-base model\n",
        "bert_model = hub.KerasLayer(encoder) # BERT-base model encoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmByjWe6wArR",
        "outputId": "9b68581f-779a-4d91-ddd1-d2cd599720d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Třetí krok:\n",
        "Tato část kódu vytváří vstupní vrstvu, vrstvu předzpracování a výstupní vrstvu modelu.\n",
        "\n",
        "```\n",
        "# text_input\n",
        "```\n",
        "`text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')`  \n",
        "vytváří vstupní vrstvu modelu s `shape=()`, což znamená, že může přijímat vstupy libovolného tvaru,   \n",
        "`dtype=tf.string`, což znamená, že může přijímat řetězcové vstupy.\n",
        "\n",
        "```\n",
        "# preprocessing\n",
        "```\n",
        "`preprocessing = bert_preprocessing_model(text_input)` aplikuje model preprocessingu.\n",
        "\n",
        "```\n",
        "# output\n",
        "```\n",
        "`output = bert_model(preprocessing)` aplikuje model BERT (`bert_model`) na předzpracovaný vstup (`preprocessing`) a získá zakódovanou reprezentaci vstupního textu.\n"
      ],
      "metadata": {
        "id": "-bHIIIB3O_eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tf.keras.layers.Input(shape= (), dtype = tf.string, name = 'text') \n",
        "preprocessing = bert_preprocessing_model(text_input)\n",
        "output = bert_model(preprocessing)"
      ],
      "metadata": {
        "id": "r9_jKZ0xO9q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####Čtvrtý krok:\n",
        "V tomto kroku vytvořte nad výstupem enkodéru BERT lineární vrstvu, což znamená, že do modelu přidává novou vrstvu po enkodéru BERT. Tato nová vrstva přijme jako vstup zakódovanou reprezentaci vstupního textu vytvořenou enkodérem BERT a před vytvořením konečného výstupu modelu na něm provede lineární transformaci. \n",
        "\n",
        "Lineární vrstva se skládá ze dvou částí:\n",
        "\n",
        "\n",
        "1.   Dropout layer: `layer_1 = tf.keras.layers.Dropout(0.1, name = 'dropout')(output['pooled_output'])` Jedná se o regularizační techniku, která pomáhá zabránit overfittingu tím, že během trénování náhodně vyřadí (nastaví na nulu) určité procento vstupních jednotek (10%). Pomáhá snížit závislost na jednom příznaku, takže se model může naučit robustnější reprezentaci příznaků.\n",
        "2.   Dense layer: `layer_2 = tf.keras.layers.Dense(len(list_classes), activation = 'sigmoid', name = 'output')(layer_1)` Tato vrstva se sigmoidní aktivační funkcí a počtem jednotek rovným počtu kategorií v cílovém atributu. Sigmoidní aktivační funkce se používá k získání pravděpodobnosti každé kategorie. Tato vrstva přebírá jako vstup výstup Dropout vrstvy. \n",
        "\n",
        "`model = tf.keras.Model(inputs = [text_input], outputs = [layer_2])`   \n",
        "Tento řádek kódu vytvoří model Keras pomocí funkčního rozhraní API. Definuje vstupy a výstupy modelu a určuje vrstvy, které budou použity k transformaci vstupů na výstupy.\n"
      ],
      "metadata": {
        "id": "oLtwXCmvR9by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_1 = tf.keras.layers.Dropout(0.1, name = 'dropout')(output['pooled_output'])\n",
        "layer_2 = tf.keras.layers.Dense(len(list_classes), activation = 'sigmoid', name = 'output')(layer_1)\n",
        "\n",
        "model = tf.keras.Model(inputs = [text_input], outputs = [layer_2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WqnASNTwBoq",
        "outputId": "8aa6865e-8083-466c-935b-a00c727910d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LaSM36MBwEY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pátý krok: \n",
        "Tato část kódu se používá k trénování modelu. Jako vstupní data použije train_data a target_data a váhy modelu se aktualizují na základě ztrátové funkce a optimalizátoru definovaného v předchozím kroku.\n",
        "\n",
        "`model.fit()` je metoda používaná k trénování modelu.\n",
        "\n",
        "Prvním parametrem jsou `train_data`, což jsou vstupní data, na kterých bude model trénován.\n",
        "\n",
        "Druhým parametrem jsou `target_data`, což jsou výstupní data neboli kategorii odpovídající vstupním datům.\n",
        "\n",
        "Třetím parametrem je `batch_size`, který určuje počet vzorků na jednu aktualizaci gradientu. V tomto případě je nastaven na 32, což znamená, že váhy modelu budou aktualizovány po každých 32 vzorcích.\n",
        "\n",
        "Čtvrtým parametrem je `epochs`, který určuje, kolikrát model projde celou sadu dat. V tomto případě je nastaven na hodnotu 3, což znamená, že model projde celou sadu dat třikrát.\n",
        "\n",
        "Posledním parametrem je `validation_split`, což je část dat, která bude použita pro validaci. V tomto případě je nastaven na 0,1, což znamená, že 10 % dat bude použito pro validaci a zbývajících 90 % bude použito pro trénování."
      ],
      "metadata": {
        "id": "dKfC6zl6eSZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning process with 3 epochs.\n",
        "model.fit(train_data, target_data, batch_size = 32, epochs = 3, validation_split = 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVJpo1UJwGCL",
        "outputId": "0305a5c3-01f3-4af0-f975-74c7cc77ad35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "310/310 [==============================] - 4540s 15s/step - loss: 0.4542 - accuracy: 0.5305 - val_loss: 0.4158 - val_accuracy: 0.6162\n",
            "Epoch 2/3\n",
            "310/310 [==============================] - 4413s 14s/step - loss: 0.4237 - accuracy: 0.5754 - val_loss: 0.3992 - val_accuracy: 0.6062\n",
            "Epoch 3/3\n",
            "310/310 [==============================] - 4489s 14s/step - loss: 0.4101 - accuracy: 0.5969 - val_loss: 0.3820 - val_accuracy: 0.6470\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb6e0a3e8e0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save and load trained model.\n",
        "model.save('сlassification_model_0.3')\n",
        "trained_model = tf.keras.models.load_model('сlassification_model_0.3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9-rd5q0wHx-",
        "outputId": "95f4bac4-5536-467b-f03b-b8dd835fcdbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 367). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    }
  ]
}